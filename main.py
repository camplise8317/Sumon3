import streamlit as st
import pandas as pd
import google.generativeai as genai
import PyPDF2
import docx
import re
import io 
import openai

# --- Configuraci√≥n de la API de Gemini y OpenAI ---
st.sidebar.header("Configuraci√≥n de API Keys")
gemini_api_key = st.sidebar.text_input("API Key de Google Gemini", type="password", help="Obt√©n tu clave en https://aistudio.google.com/app/apikey")
openai_api_key = st.sidebar.text_input("API Key de OpenAI (para modelos GPT)", type="password", help="Obt√©n tu clave en https://platform.openai.com/account/api-keys")

# Inicializaci√≥n condicional de Gemini y OpenAI
gemini_config_ok = False
openai_config_ok = False

if gemini_api_key:
    try:
        genai.configure(api_key=gemini_api_key)
        gemini_config_ok = True
        st.sidebar.success("API Key de Gemini configurada.")
    except Exception as e:
        st.sidebar.error(f"Error al configurar la API Key de Gemini: {e}")
else:
    st.sidebar.warning("Por favor, ingresa tu API Key de Gemini para usar modelos Gemini.")

if openai_api_key:
    openai.api_key = openai_api_key
    openai_config_ok = True
    st.sidebar.success("API Key de OpenAI configurada.")
else:
    st.sidebar.warning("Por favor, ingresa tu API Key de OpenAI para usar modelos GPT.")

# --- Funciones de Lectura de Archivos (Adaptadas para Streamlit Uploader) ---
@st.cache_data 
def leer_excel_cargado(uploaded_file):
    """
    Lee un archivo Excel cargado por Streamlit y lo carga en un DataFrame de pandas.
    """
    if uploaded_file is not None:
        try:
            df = pd.read_excel(uploaded_file)
            st.sidebar.success(f"Archivo Excel '{uploaded_file.name}' cargado exitosamente.")
            return df
        except Exception as e:
            st.sidebar.error(f"Ocurri√≥ un error al leer el archivo Excel: {e}")
            return None
    return None

@st.cache_data 
def leer_pdf_cargado(uploaded_file):
    """
    Lee el texto de un archivo PDF cargado por Streamlit.
    """
    if uploaded_file is not None:
        try:
            texto_pdf = ""
            reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
            for page_num in range(len(reader.pages)):
                texto_pdf += reader.pages[page_num].extract_text()
            st.sidebar.success(f"Archivo PDF '{uploaded_file.name}' le√≠do exitosamente.")
            return texto_pdf
        except Exception as e:
            st.sidebar.error(f"Ocurri√≥ un error al leer el archivo PDF: {e}")
            return ""
    return ""

# --- Funci√≥n para obtener la descripci√≥n de la taxonom√≠a de Bloom ---
def get_descripcion_bloom(proceso_cognitivo_elegido):
    descripcion_bloom_map = {
        "RECORDAR": "Recuperar informaci√≥n relevante desde la memoria de largo plazo.",
        "COMPRENDER": "Construir significado a partir de informaci√≥n mediante interpretaci√≥n, resumen, explicaci√≥n u otras tareas.",
        "APLICAR": "Usar procedimientos en situaciones conocidas o nuevas.",
        "ANALIZAR": "Descomponer informaci√≥n y examinar relaciones entre partes.",
        "EVALUAR": "Emitir juicios basados en criterios para valorar ideas o soluciones.",
        "CREAR": "Generar nuevas ideas, productos o formas de reorganizar informaci√≥n."
    }
    return descripcion_bloom_map.get(str(proceso_cognitivo_elegido).upper(), "Descripci√≥n no disponible para este proceso cognitivo.")

# --- Funci√≥n para generar texto con Gemini o GPT ---
def generar_texto_con_llm(model_type, model_name, prompt):
    if model_type == "Gemini":
        if not gemini_config_ok:
            st.error("API Key de Gemini no configurada. No se puede generar texto con Gemini.")
            return None
        try:
            modelo = genai.GenerativeModel(model_name)
            response = modelo.generate_content(prompt)
            return response.text
        except Exception as e:
            st.error(f"Error al llamar a Gemini: {e}")
            return None
    elif model_type == "GPT":
        if not openai_config_ok:
            st.error("API Key de OpenAI no configurada. No se puede generar texto con GPT.")
            return None
        try:
            client = openai.OpenAI(api_key=openai.api_key)
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000 
            )
            return response.choices[0].message.content
        except Exception as e:
            st.error(f"Error al llamar a OpenAI: {e}")
            return None
    return None

# --- Funci√≥n para auditar el √≠tem generado ---
def auditar_item_con_llm(model_type, model_name, item_generado, grado, area, asignatura, estacion, 
                         proceso_cognitivo, nanohabilidad, microhabilidad, 
                         competencia_nanohabilidad, contexto_educativo, manual_reglas_texto="", descripcion_bloom="", grafico_necesario="", descripcion_grafico="", prompt_auditor_adicional=""):
    """
    Audita un √≠tem generado para verificar su cumplimiento con criterios espec√≠ficos.
    """
    auditoria_prompt = f"""
    Eres un experto en validaci√≥n de √≠tems educativos, especializado en pruebas tipo ICFES y las directrices del equipo IMPROVE.
    Tu tarea es AUDITAR RIGUROSAMENTE el siguiente √≠tem generado por un modelo de lenguaje.

    Debes verificar que el √≠tem cumpla con TODOS los siguientes criterios, prestando especial atenci√≥n a la alineaci√≥n con los par√°metros proporcionados y a las reglas de formato y contenido.

    --- CRITERIOS DE AUDITOR√çA ---
    1.  **Formato del Enunciado:** ¬øEl enunciado est√° formulado como pregunta clara y directa, sin ambig√ºedades ni errores?
    2.  **N√∫mero de Opciones:** ¬øHay exactamente 4 opciones (A, B, C, D)?
    3.  **Respuesta Correcta Indicada:** ¬øLa secci√≥n 'RESPUESTA CORRECTA:' est√° claramente indicada y coincide con una de las opciones?
    4.  **Dise√±o de Justificaciones:**
        * ¬øHay justificaciones bien diferenciadas para CADA opci√≥n (A, B, C, D)?
        * ¬øLa justificaci√≥n de la opci√≥n **correcta** explica el razonamiento, procedimiento o estrategia relevante (NO por descarte)?
        * ¬øLas justificaciones de las opciones **incorrectas** est√°n redactadas siguiendo el formato: ‚ÄúEl estudiante podr√≠a escoger la opci√≥n X porque‚Ä¶ Sin embargo, esto es incorrecto porque‚Ä¶‚Äù?
    5.  **Estilo y Restricciones:** ¬øNo se usan negaciones mal redactadas, nombres reales, marcas, lugares reales, datos personales o frases vagas como ‚Äúninguna de las anteriores‚Äù o ‚Äútodas las anteriores‚Äù?
    6.  **Alineaci√≥n del Contenido:** ¬øEl √≠tem (contexto, enunciado, opciones) est√° alineado EXCLUSIVAMENTE con los siguientes elementos tem√°ticos y cognitivos?
        * Grado: {grado}
        * √Årea: {area}
        * Asignatura: {asignatura}
        * Estaci√≥n o unidad tem√°tica: {estacion}
        * Proceso Cognitivo (Taxonom√≠a de Bloom): {proceso_cognitivo} (su descripci√≥n es "{descripcion_bloom}")
        * Nanohabilidad (foco principal): {nanohabilidad}
        * Microhabilidad (evidencia de aprendizaje): {microhabilidad}
        * Competencia (asociada a Nanohabilidad): {competencia_nanohabilidad}
        * Nivel educativo del estudiante: {contexto_educativo}
    7.  **Gr√°fico (si aplica):** Si el √≠tem indica que requiere un gr√°fico, ¬øla descripci√≥n del gr√°fico es clara, detallada y funcional para su futura creaci√≥n?
        * Gr√°fico Necesario: {grafico_necesario}
        * Descripci√≥n del Gr√°fico: {descripcion_grafico if grafico_necesario == 'S√ç' else 'N/A'}

    --- MANUAL DE REGLAS ADICIONAL ---
    Las siguientes reglas son de suma importancia para la calidad y pertinencia del √≠tem. Debes asegurar que el √≠tem cumple con todas ellas.
    {manual_reglas_texto}
    -----------------------------------

    --- INSTRUCCIONES ADICIONALES PARA LA AUDITOR√çA ---
    {prompt_auditor_adicional if prompt_auditor_adicional else "No se proporcionaron instrucciones adicionales para la auditor√≠a."}
    ---------------------------------------------------

    √çTEM A AUDITAR:
    --------------------
    {item_generado}
    --------------------

    Devuelve tu auditor√≠a con este formato estructurado:

    VALIDACI√ìN DE CRITERIOS:
    - Formato del Enunciado: [‚úÖ / ‚ùå] + Comentario (si ‚ùå)
    - N√∫mero de Opciones (4): [‚úÖ / ‚ùå]
    - Respuesta Correcta Indicada: [‚úÖ / ‚ùå]
    - Dise√±o de Justificaciones: [‚úÖ / ‚ö†Ô∏è / ‚ùå] + Observaciones (si ‚ö†Ô∏è/‚ùå)
    - Estilo y Restricciones: [‚úÖ / ‚ö†Ô∏è / ‚ùå] + Observaciones (si ‚ö†Ô∏è/‚ùå)
    - Alineaci√≥n del Contenido: [‚úÖ / ‚ùå] + Comentario (si ‚ùå)
    - Gr√°fico (si aplica): [‚úÖ / ‚ö†Ô∏è / ‚ùå] + Observaciones (si ‚ö†Ô∏è/‚ùå)

    DICTAMEN FINAL:
    [‚úÖ CUMPLE TOTALMENTE / ‚ö†Ô∏è CUMPLE PARCIALMENTE / ‚ùå RECHAZADO]

    OBSERVACIONES FINALES:
    [Explica de forma concisa qu√© aspectos necesitan mejora, si el dictamen no es ‚úÖ. Si es ‚úÖ, puedes indicar "El √≠tem cumple con todos los criterios."]
    """
    return generar_texto_con_llm(model_type, model_name, auditoria_prompt), auditoria_prompt # Retorna tambi√©n el prompt de auditor√≠a

# --- Funci√≥n para generar preguntas usando el modelo de generaci√≥n seleccionado ---
def generar_pregunta_con_seleccion(gen_model_type, gen_model_name, audit_model_type, audit_model_name, 
                                     fila_datos, criterios_generacion, manual_reglas_texto="", 
                                     informacion_adicional_usuario="", 
                                     prompt_bloom_adicional="", prompt_construccion_adicional="", prompt_especifico_adicional="", 
                                     prompt_auditor_adicional="",
                                     contexto_general_estacion="", feedback_usuario=""): # A√±ade el feedback del usuario
    """
    Genera una pregunta educativa de opci√≥n m√∫ltiple usando el modelo de generaci√≥n seleccionado
    y la itera para refinarla si la auditor√≠a lo requiere.
    """
    tipo_pregunta = criterios_generacion.get("tipo_pregunta", "opci√≥n m√∫ltiple con 4 opciones") 
    dificultad = criterios_generacion.get("dificultad", "media")
    contexto_educativo = criterios_generacion.get("contexto_educativo", "general")
    formato_justificacion = criterios_generacion.get("formato_justificacion", """
        ‚Ä¢ Justificaci√≥n correcta: debe explicar el razonamiento o proceso cognitivo (NO por descarte).
        ‚Ä¢ Justificaciones incorrectas: deben redactarse como: ‚ÄúEl estudiante podr√≠a escoger la opci√≥n X porque‚Ä¶ Sin embargo, esto es incorrecto porque‚Ä¶‚Äù
    """)
    
    grado_elegido = fila_datos.get('GRADO', 'no especificado')
    area_elegida = fila_datos.get('√ÅREA', 'no especificada')
    asignatura_elegida = fila_datos.get('ASIGNATURA', 'no especificada')
    estacion_elegida = fila_datos.get('ESTACI√ìN', 'no especificada')
    proceso_cognitivo_elegido = fila_datos.get('PROCESO COGNITIVO', 'no especificado')
    nanohabilidad_elegida = fila_datos.get('NANOHABILIDAD', 'no especificada')
    microhabilidad_elegida = fila_datos.get('MICROHABILIDAD', 'no especificada')
    competencia_nanohabilidad_elegida = fila_datos.get('COMPETENCIA NANOHABILIDAD', 'no especificada')

    dato_para_pregunta_foco = nanohabilidad_elegida
    descripcion_bloom = get_descripcion_bloom(proceso_cognitivo_elegido)

    current_item_text = ""
    auditoria_status = "‚ùå RECHAZADO" # Estado inicial
    audit_observations = "" # Observaciones para el refinamiento
    max_refinement_attempts = 5 # N√∫mero m√°ximo de intentos de refinamiento
    attempt = 0
    grafico_necesario = "NO" # Valor por defecto
    descripcion_grafico = "" # Valor por defecto

    # Almacenar detalles de clasificaci√≥n para el √≠tem
    classification_details = {
        "Grado": grado_elegido,
        "√Årea": area_elegida,
        "Asignatura": asignatura_elegida,
        "Estaci√≥n": estacion_elegida,
        "Proceso Cognitivo": proceso_cognitivo_elegido,
        "Nanohabilidad": nanohabilidad_elegida,
        "Microhabilidad": microhabilidad_elegida,
        "Competencia Nanohabilidad": competencia_nanohabilidad_elegida
    }

    item_final_data = None # Variable para guardar el √≠tem final (aprobado o la √∫ltima versi√≥n auditada)
    full_generation_prompt = "" # Variable para almacenar el prompt completo del generador
    full_auditor_prompt = "" # Variable para almacenar el prompt completo del auditor

    # A√±ade el feedback del usuario al prompt principal del generador
    prompt_con_feedback = ""
    if feedback_usuario:
        prompt_con_feedback = f"--- RETROALIMENTACI√ìN DE USUARIO PARA REFINAMIENTO ---\n{feedback_usuario}\n---------------------------------------------------"

    while auditoria_status != "‚úÖ CUMPLE TOTALMENTE" and attempt < max_refinement_attempts:
        attempt += 1
        # st.info(f"--- Generando/Refinando √çtem (Intento {attempt}/{max_refinement_attempts}) ---") # Comentado para no saturar si son muchos √≠tems

        # Construcci√≥n del prompt para el GENERADOR
        prompt_content_for_llm = f"""
        Eres un dise√±ador experto en √≠tems de evaluaci√≥n educativa, especializado en pruebas tipo ICFES u otras de alta calidad t√©cnica.

        Tu tarea es construir un √≠tem de {tipo_pregunta} con una √∫nica respuesta correcta, cumpliendo rigurosamente las reglas de construcci√≥n de √≠tems y alineado con el marco cognitivo de la Taxonom√≠a de Bloom.

        --- CONTEXTO Y PAR√ÅMETROS DEL √çTEM ---
        - Grado: {grado_elegido}
        - √Årea: {area_elegida}
        - Asignatura: {asignatura_elegida}
        - Estaci√≥n o unidad tem√°tica: {estacion_elegida}
        - Proceso cognitivo (Taxonom√≠a de Bloom): {proceso_cognitivo_elegido}
        - Descripci√≥n del proceso cognitivo:
          "{descripcion_bloom}"
        
        --- PROMPT ADICIONAL: TAXONOM√çA DE BLOOM / PROCESOS COGNITIVOS ---
        {prompt_bloom_adicional if prompt_bloom_adicional else "No se proporcionaron prompts adicionales espec√≠ficos para taxonom√≠a de Bloom."}
        ------------------------------------------------------------------

        - Nanohabilidad (foco principal del √≠tem): {nanohabilidad_elegida}
        - Nivel educativo esperado del estudiante: {contexto_educativo}
        - Nivel de dificultad deseado: {dificultad}

        --- CONTEXTO GENERAL DE LA ESTACI√ìN (si aplica) ---
        {f"Considera este contexto general para todos los √≠tems de esta estaci√≥n: {contexto_general_estacion}" if contexto_general_estacion else "Este √≠tem debe generar su propio contexto individual, o no se ha definido un contexto general para la estaci√≥n."}
        ----------------------------------------------------

        --- INSTRUCCIONES PARA LA CONSTRUCCI√ìN DEL √çTEM ---
        CONTEXTO DEL √çTEM:
        - Incluye una situaci√≥n contextualizada, relevante y plausible para el grado y √°rea indicada.
        - La tem√°tica debe ser la de la {estacion_elegida}, y esto debe ser central, no una mera contextualizaci√≥n.
        - Debe garantizarse que el proceso cognitivo corresponde fielmente a la descripci√≥n de la taxonomia de Bloom.
        - Evita referencias a marcas, nombres propios, lugares reales o informaci√≥n personal identificable.

        ENUNCIADO:
        - Formula una pregunta clara, directa, sin ambig√ºedades ni tecnicismos innecesarios.
        - Si utilizas negaciones, res√°ltalas en MAY√öSCULAS Y NEGRITA (por ejemplo: **NO ES**, **EXCEPTO**).
        - Aseg√∫rate de que el enunciado refleje el tipo de tarea cognitiva esperado seg√∫n el proceso de Bloom.

        OPCIONES DE RESPUESTA:
        - Escribe exactamente cuatro opciones (A, B, C  y D).
        - Solo una opci√≥n debe ser correcta.
        - Los distractores (respuestas incorrectas) deben estar bien dise√±ados: deben ser cre√≠bles, funcionales y representar errores comunes o concepciones alternativas frecuentes.
        - No utilices f√≥rmulas vagas como ‚Äúninguna de las anteriores‚Äù o ‚Äútodas las anteriores‚Äù.

        JUSTIFICACIONES:
        {formato_justificacion}

        --- PROMPT ADICIONAL: REGLAS GENERALES DE CONSTRUCCI√ìN ---
        {prompt_construccion_adicional if prompt_construccion_adicional else "No se proporcionaron prompts adicionales espec√≠ficos para reglas generales de construcci√≥n."}
        ---------------------------------------------------------

        --- REGLAS ADICIONALES DEL MANUAL DE CONSTRUCCI√ìN ---
        Considera y aplica estrictamente todas las directrices, ejemplos y restricciones contenidas en el siguiente manual.
        Esto es de suma importancia para la calidad y pertinencia del √≠tem.

        Manual de Reglas:
        {manual_reglas_texto}
        ----------------------------------------------------

        --- INFORMACI√ìN ADICIONAL PROPORCIONADA POR EL USUARIO (Contexto General) ---
        {informacion_adicional_usuario if informacion_adicional_usuario else "No se proporcion√≥ informaci√≥n adicional general."}
        ---------------------------------------------------------------------------
        
        --- PROMPT ADICIONAL: COSAS ESPEC√çFICAS A TENER EN CUENTA ---
        {prompt_especifico_adicional if prompt_especifico_adicional else "No se proporcionaron prompts adicionales espec√≠ficos para consideraciones adicionales."}
        ----------------------------------------------------------

        --- DATO CLAVE PARA LA CONSTRUCCI√ìN ---
        Basado en el foco tem√°tico y el proceso cognitivo, considera el siguiente dato o idea esencial:
        "{dato_para_pregunta_foco}"

        --- INSTRUCCIONES ESPEC√çFICAS DE SALIDA PARA GR√ÅFICO ---
        Despu√©s del bloque de JUSTIFICACIONES, incluye la siguiente informaci√≥n para indicar si el √≠tem necesita un gr√°fico y c√≥mo ser√≠a:
        GRAFICO_NECESARIO: [S√ç/NO]
        DESCRIPCION_GRAFICO: [Si GRAFICO_NECESARIO es S√ç, proporciona una descripci√≥n MUY DETALLADA del gr√°fico. Incluye: tipo de gr√°fico (ej. barras, l√≠neas, circular, diagrama de flujo, imagen de un objeto), datos o rangos de valores, etiquetas de ejes, elementos clave, prop√≥sito del gr√°fico y c√≥mo se relaciona con la pregunta. Si es NO, escribe N/A.]

        --- FORMATO ESPERADO DE SALIDA ---
        PREGUNTA: [Redacta aqu√≠ el enunciado de la pregunta]
        A. [Opci√≥n A]  
        B. [Opci√≥n B]  
        C. [Opci√≥n C] 
        D. [Opci√≥n D]          
        RESPUESTA CORRECTA: [Letra de la opci√≥n correcta, por ejemplo: B]
        JUSTIFICACIONES:  
        A. [Explica por qu√© A es incorrecta o correcta]  
        B. [Explica por qu√© B es incorrecta o correcta]  
        C. [Explica por qu√© C es incorrecta o correcta]  
        D. [Explica por qu√© D es incorrecta o correcta]  
        GRAFICO_NECESARIO: [S√ç/NO]
        DESCRIPCION_GRAFICO: [Descripci√≥n detallada o N/A]
        """
        
        # Si no es el primer intento, a√±ade las observaciones de auditor√≠a para refinamiento
        if attempt > 1:
            prompt_content_for_llm += f"""
            --- RETROALIMENTACI√ìN DE AUDITOR√çA PARA REFINAMIENTO ---
            El √≠tem anterior no cumpli√≥ con todos los criterios. Por favor, revisa las siguientes observaciones y mejora el √≠tem para abordarlas.
            Observaciones del Auditor:
            {audit_observations}
            ---------------------------------------------------
            """
            # Agrega el √≠tem anterior para que el LLM lo pueda reformular
            prompt_content_for_llm += f"""
            --- √çTEM ANTERIOR A REFINAR ---
            {current_item_text}
            -------------------------------
            """
        
        # A√±ade el prompt de feedback del usuario si existe
        prompt_content_for_llm += prompt_con_feedback
        
        # Guardar el prompt completo del generador antes de enviarlo
        full_generation_prompt = prompt_content_for_llm

        try:
            with st.spinner(f"Generando contenido con IA ({gen_model_type} - {gen_model_name}, Intento {attempt})..."):
                full_llm_response = generar_texto_con_llm(gen_model_type, gen_model_name, prompt_content_for_llm)
                
                if full_llm_response is None: # Si hubo un error en la generaci√≥n con LLM
                    # st.error(f"Fallo en la generaci√≥n de texto con {gen_model_type} ({gen_model_name}).") # Comentado para no saturar si son muchos √≠tems
                    auditoria_status = "‚ùå RECHAZADO (Error de Generaci√≥n)"
                    audit_observations = "El modelo de generaci√≥n no pudo producir una respuesta v√°lida."
                    break # Salir del bucle de refinamiento
                    
                # --- Parsear la respuesta para extraer el √≠tem y la informaci√≥n del gr√°fico ---
                item_and_graphic_match = re.search(r"(PREGUNTA:.*?)(GRAFICO_NECESARIO:\s*(S√ç|NO).*?DESCRIPCION_GRAFICO:.*)", full_llm_response, re.DOTALL)
                
                if item_and_graphic_match:
                    current_item_text = item_and_graphic_match.group(1).strip()
                    grafico_info_block = item_and_graphic_match.group(2).strip()
                    
                    grafico_necesario_match = re.search(r"GRAFICO_NECESARIO:\s*(S√ç|NO)", grafico_info_block)
                    if grafico_necesario_match:
                        grafico_necesario = grafico_necesario_match.group(1).strip()

                    descripcion_grafico_match = re.search(r"DESCRIPCION_GRAFICO:\s*(.*)", grafico_info_block, re.DOTALL)
                    if descripcion_grafico_match:
                        descripcion_grafico = descripcion_grafico_match.group(1).strip()
                        if descripcion_grafico.upper() == 'N/A':
                            descripcion_grafico = ""
                else:
                    current_item_text = full_llm_response
                    grafico_necesario = "NO"
                    descripcion_grafico = ""
                    st.warning("No se pudo parsear el formato de gr√°fico de la respuesta. Asumiendo que no requiere gr√°fico.")
                
            with st.spinner(f"Auditando √≠tem ({audit_model_type} - {audit_model_name}, Intento {attempt})..."):
                auditoria_resultado, full_auditor_prompt = auditar_item_con_llm( # Recibe tambi√©n el prompt del auditor
                    audit_model_type, audit_model_name,
                    item_generado=current_item_text,
                    grado=grado_elegido, area=area_elegida, asignatura=asignatura_seleccionada, estacion=estacion_elegida,
                    proceso_cognitivo=proceso_cognitivo_seleccionado, nanohabilidad=nanohabilidad_seleccionada,
                    microhabilidad=microhabilidad_elegida, competencia_nanohabilidad=competencia_nanohabilidad_elegida,
                    contexto_educativo=contexto_educativo, manual_reglas_texto=manual_reglas_texto,
                    descripcion_bloom=descripcion_bloom,
                    grafico_necesario=grafico_necesario,
                    descripcion_grafico=descripcion_grafico,
                    prompt_auditor_adicional=prompt_auditor_adicional # Pasa el prompt adicional del auditor
                )
                if auditoria_resultado is None: # Si hubo un error en la auditor√≠a con LLM
                    # st.error(f"Fallo en la auditor√≠a con {audit_model_type} ({audit_model_name}).") # Comentado
                    auditoria_status = "‚ùå RECHAZADO (Error de Auditor√≠a)"
                    audit_observations = "El modelo de auditor√≠a no pudo producir una respuesta v√°lida."
                    break # Salir del bucle de refinamiento

                # --- Extraer DICTAMEN FINAL de forma m√°s robusta ---
                dictamen_final_match = re.search(r"DICTAMEN FINAL:\s*\[(.*?)]", auditoria_resultado, re.DOTALL)
                if dictamen_final_match:
                    auditoria_status = dictamen_final_match.group(1).strip()
                else:
                    auditoria_status = "‚ùå RECHAZADO (no se pudo extraer dictamen)"
                
                observaciones_start = auditoria_resultado.find("OBSERVACIONES FINALES:")
                if observaciones_start != -1:
                    audit_observations = auditoria_resultado[observaciones_start + len("OBSERVACIONES FINALES:"):].strip()
                else:
                    audit_observations = "No se pudieron extraer observaciones espec√≠ficas del auditor. Posiblemente un error de formato en la respuesta del auditor."
                
            # Guardar los datos del √≠tem, incluyendo el estado final de la auditor√≠a y observaciones
            item_final_data = {
                "item_text": current_item_text,
                "classification": classification_details,
                "grafico_necesario": grafico_necesario,
                "descripcion_grafico": descripcion_grafico,
                "final_audit_status": auditoria_status, 
                "final_audit_observations": audit_observations,
                "generation_prompt_used": full_generation_prompt, # Guarda el prompt exacto usado por el generador
                "auditor_prompt_used": full_auditor_prompt
            }

            if auditoria_status == "‚úÖ CUMPLE TOTALMENTE":
                break # Sale del ciclo de refinamiento si es aprobado
            else:
                pass # Solo se registra el estado, no se muestra advertencia por cada intento en bucle masivo

        except Exception as e:
            audit_observations = f"Error t√©cnico durante la generaci√≥n: {e}. Por favor, corrige este problema."
            auditoria_status = "‚ùå RECHAZADO (error t√©cnico)"  
            item_final_data = {
                "item_text": current_item_text if current_item_text else "No se pudo generar el √≠tem debido a un error t√©cnico.",
                "classification": classification_details,
                "grafico_necesario": "NO",
                "descripcion_grafico": "",
                "final_audit_status": auditoria_status,
                "final_audit_observations": audit_observations,
                "generation_prompt_used": full_generation_prompt,
                "auditor_prompt_used": full_auditor_prompt
            }
            break # Salir del ciclo si hay un error t√©cnico grave

    if item_final_data is None:  
        return None # Retorna None si no se logr√≥ generar nada en absoluto.

    return item_final_data # Retorna el diccionario del √≠tem procesado

# --- Funci√≥n para exportar preguntas a un documento Word ---
def exportar_a_word(preguntas_procesadas_list):
    """
    Exporta una lista de preguntas procesadas a un documento de Word (.docx) en memoria,
    incluyendo sus detalles de clasificaci√≥n, la descripci√≥n del gr√°fico si aplica,
    y el dictamen final de la auditor√≠a.
    Returns: BytesIO object of the document.
    """
    doc = docx.Document()
    
    doc.add_heading('Preguntas Generadas y Auditadas', level=1)
    doc.add_paragraph('Este documento contiene los √≠tems generados por el sistema de IA y sus resultados de auditor√≠a.')
    doc.add_paragraph('') # Espacio en blanco

    if not preguntas_procesadas_list:
        doc.add_paragraph('No se procesaron √≠tems para este informe.')

    for i, item_data in enumerate(preguntas_procesadas_list):
        pregunta_texto = item_data["item_text"]
        classification = item_data["classification"]
        grafico_necesario = item_data.get("grafico_necesario", "NO")
        descripcion_grafico = item_data.get("descripcion_grafico", "")
        final_audit_status = item_data.get("final_audit_status", "N/A")
        final_audit_observations = item_data.get("final_audit_observations", "No hay observaciones finales de auditor√≠a.")

        doc.add_heading(f'√çtem #{i+1}', level=2)
        
        # A√±adir detalles de clasificaci√≥n
        doc.add_paragraph('--- Clasificaci√≥n del √çtem ---') 
        for key, value in classification.items():
            p = doc.add_paragraph()
            run = p.add_run(f"{key}: ")
            run.bold = True
            p.add_run(str(value)) 

        doc.add_paragraph('') 
        
        # A√±adir el texto de la pregunta y su formato
        lines = pregunta_texto.split('\n')
        for line in lines:
            line = line.strip() 
            if not line: 
                continue

            if line.startswith("PREGUNTA:"):
                p = doc.add_paragraph()
                run = p.add_run(line)
                run.bold = True
                run.font.size = docx.shared.Pt(12) 
            elif line.startswith("A.") or line.startswith("B.") or line.startswith("C.")or line.startswith("D."):
                p = doc.add_paragraph(line)
                p.paragraph_format.left_indent = docx.shared.Inches(0.5) 
            elif line.startswith("RESPUESTA CORRECTA:"):
                p = doc.add_paragraph()
                run = p.add_run(line)
                run.bold = True
            elif line.startswith("JUSTIFICACIONES:"):
                p = doc.add_paragraph()
                run = p.add_run(line)
                run.bold = True
            elif line.startswith("GRAFICO_NECESARIO:") or line.startswith("DESCRIPCION_GRAFICO:"):
                continue 
            elif line.startswith("VALIDACI√ìN DE CRITERIOS:") or line.startswith("DICTAMEN FINAL:") or line.startswith("OBSERVACIONES FINALES:"):
                p = doc.add_paragraph()
                run = p.add_run(line)
                run.bold = True
            elif line.startswith("‚úÖ") or line.startswith("‚ö†Ô∏è") or line.startswith("‚ùå"):
                p = doc.add_paragraph(line)
                p.paragraph_format.left_indent = docx.shared.Inches(0.25) 
            else:
                doc.add_paragraph(line)
        
        # A√±adir descripci√≥n del gr√°fico si es necesario
        if grafico_necesario == "S√ç" and descripcion_grafico:
            doc.add_paragraph('')
            p = doc.add_paragraph()
            run = p.add_run("--- Gr√°fico Sugerido ---")
            run.bold = True
            doc.add_paragraph(f"**Tipo y Descripci√≥n del Gr√°fico:** {descripcion_grafico}")
            doc.add_paragraph('') 

        # A√±adir el dictamen final y las observaciones de la auditor√≠a para CADA √≠tem
        doc.add_paragraph('')
        p = doc.add_paragraph()
        run = p.add_run("--- Resultado Final de Auditor√≠a ---")
        run.bold = True
        doc.add_paragraph(f"**DICTAMEN FINAL:** {final_audit_status}")
        doc.add_paragraph(f"**OBSERVACIONES FINALES:** {final_audit_observations}")
        doc.add_paragraph('') 

        doc.add_page_break() 

    # Guardar el documento en un buffer en memoria
    buffer = io.BytesIO()
    doc.save(buffer)
    buffer.seek(0) 
    return buffer

# --- Interfaz de Usuario Principal de Streamlit ---
st.title("üìö Generador y Auditor de √≠tems para el proyecto SUMUN üß†")
st.markdown("Esta aplicaci√≥n genera √≠tems de selecci√≥n m√∫ltiple y audita su calidad, permiti√©ndote guiar a la IA con prompts adicionales.")

# Secci√≥n de Carga de Archivos Global (Excel y PDF)
st.sidebar.header("Carga de Archivos Global")
uploaded_excel_file = st.sidebar.file_uploader("Sube tu archivo Excel (ESTRUCTURA_TOTAL.xlsx)", type=["xlsx"])
uploaded_pdf_file = st.sidebar.file_uploader("Sube tu archivo PDF (Manual_construccion_pruebas_IMProve.pdf)", type=["pdf"])

df_datos = None
manual_reglas_texto = ""

if uploaded_excel_file:
    df_datos = leer_excel_cargado(uploaded_excel_file)

if uploaded_pdf_file:
    manual_reglas_texto = leer_pdf_cargado(uploaded_pdf_file)
    max_manual_length = 15000 
    if len(manual_reglas_texto) > max_manual_length:
        st.sidebar.warning(f"Manual es demasiado largo ({len(manual_reglas_texto)} caracteres). Truncando a {max_manual_length} caracteres para la IA.")
        manual_reglas_texto = manual_reglas_texto[:max_manual_length] # Corregido typo
    st.sidebar.info(f"Manual de reglas cargado. Longitud final: {len(manual_reglas_texto)} caracteres.")

# --- L√≥gica principal de Generaci√≥n y Auditor√≠a de √çtems ---
st.header("Generaci√≥n y Auditor√≠a de √çtems.")
st.markdown("Define los criterios del √≠tem y utiliza modelos de IA para generarlo y validarlo.")

if df_datos is None:
    st.info("Para comenzar, por favor sube tu archivo **Excel** con la estructura de datos en la barra lateral.")
elif not (gemini_config_ok or openai_config_ok):
    st.info("Para usar los modelos de IA, por favor ingresa al menos una **API Key de Gemini o OpenAI** en la barra lateral.")
else:
    st.subheader("Selecciona los Criterios para la Generaci√≥n")

    # Obtener valores √∫nicos para cada columna para los selectbox
    all_grades = df_datos['GRADO'].dropna().unique().tolist()
    grado_seleccionado = st.selectbox("Grado", sorted(all_grades), key="grado_sel")

    # Filtrar el DataFrame seg√∫n la selecci√≥n del grado
    df_filtrado_grado = df_datos[df_datos['GRADO'].astype(str).str.upper() == str(grado_seleccionado).upper()]
    all_areas = df_filtrado_grado['√ÅREA'].dropna().unique().tolist()
    area_seleccionada = st.selectbox("√Årea", sorted(all_areas), key="area_sel")

    # Filtrar seg√∫n la selecci√≥n del √°rea
    df_filtrado_area = df_filtrado_grado[df_filtrado_grado['√ÅREA'].astype(str).str.upper() == str(area_seleccionada).upper()]
    all_asignaturas = df_filtrado_area['ASIGNATURA'].dropna().unique().tolist()
    asignatura_seleccionada = st.selectbox("Asignatura", sorted(all_asignaturas), key="asignatura_sel")

    # Filtrar seg√∫n la selecci√≥n de asignatura
    df_filtrado_asignatura = df_filtrado_area[df_filtrado_area['ASIGNATURA'].astype(str).str.upper() == str(asignatura_seleccionada).upper()]
    all_estaciones = df_filtrado_asignatura['ESTACI√ìN'].dropna().unique().tolist()
    estacion_seleccionada = st.selectbox("Estaci√≥n", sorted(all_estaciones), key="estacion_sel")

    # Filtrar seg√∫n la selecci√≥n de estaci√≥n
    df_filtrado_estacion = df_filtrado_asignatura[df_filtrado_asignatura['ESTACI√ìN'].astype(str).str.upper() == str(estacion_seleccionada).upper()]
    
    # --- Nueva L√≥gica: Generar todos los √≠tems de la estaci√≥n o uno espec√≠fico ---
    st.markdown("---")
    st.subheader("Modo de Generaci√≥n de √çtems")
    generate_all_for_station = st.checkbox(
        "Generar TODOS los √≠tems de esta Estaci√≥n (uno por cada Proceso Cognitivo asociado)",
        key="generate_all_station"
    )

    contexto_general_estacion = ""
    if generate_all_for_station:
        st.info("Has seleccionado generar √≠tems para todos los procesos cognitivos asociados a esta estaci√≥n.")
        st.markdown("##### Configuraci√≥n del Contexto General para la Estaci√≥n")
        context_option = st.radio(
            "¬øC√≥mo deseas establecer el contexto general para todos los √≠tems de esta estaci√≥n?",
            ("Yo quiero dar una idea del contexto general", "Quiero que el contexto general sea generado por la IA"),
            key="context_gen_option",
            horizontal=True
        )

        if context_option == "Yo quiero dar una idea del contexto general":
            contexto_general_estacion = st.text_area(
                "Escribe tu idea o directriz para el contexto general de la estaci√≥n (ej: 'Un viaje escolar a un ecosistema de p√°ramo'):",
                height=150, key="user_context_idea"
            )
            if not contexto_general_estacion.strip():
                st.warning("Por favor, introduce una idea para el contexto general o selecciona que la IA lo genere.")
        else:
            st.info("La IA generar√° un contexto general apropiado para esta estaci√≥n.")
            # No se necesita input del usuario, el LLM lo generar√° internamente.

    # Filtrar para proceso cognitivo y nanohabilidad SOLO SI NO SE GENERAN TODOS LOS DE LA ESTACI√ìN
    proceso_cognitivo_seleccionado = None
    nanohabilidad_seleccionada = None
    df_item_seleccionado = None

    if not generate_all_for_station:
        all_procesos = df_filtrado_estacion['PROCESO COGNITIVO'].dropna().unique().tolist()
        proceso_cognitivo_seleccionado = st.selectbox("Proceso Cognitivo", sorted(all_procesos), key="proceso_sel")

        df_filtrado_proceso = df_filtrado_estacion[df_filtrado_estacion['PROCESO COGNITIVO'].astype(str).str.upper() == str(proceso_cognitivo_seleccionado).upper()]
        all_nanohabilidades = df_filtrado_proceso['NANOHABILIDAD'].dropna().unique().tolist()
        nanohabilidad_seleccionada = st.selectbox("Nanohabilidad", sorted(all_nanohabilidades), key="nanohabilidad_sel")

        # Se filtra el DataFrame final para un solo √≠tem
        df_item_seleccionado = df_filtrado_proceso[df_filtrado_proceso['NANOHABILIDAD'].astype(str).str.upper() == str(nanohabilidad_seleccionada).upper()]
        
        if df_item_seleccionado.empty:
            st.warning("No se encontraron datos en el Excel para la combinaci√≥n de criterios seleccionada. Por favor, ajusta tus filtros.")
            # st.stop() # No detener la ejecuci√≥n, solo mostrar el mensaje
    else: # Si se generan todos, necesitamos las filas de la estaci√≥n completa
        df_item_seleccionado = df_filtrado_estacion.copy() # Copiamos todas las filas de la estaci√≥n


    if df_item_seleccionado.empty:
        st.error("No hay datos v√°lidos para generar √≠tems con los filtros actuales. Por favor, revisa tus selecciones o el archivo Excel.")
    else: # Solo mostrar opciones de generaci√≥n si hay datos v√°lidos
        # --- Informaci√≥n Adicional del Usuario (contexto general para el √≠tem, si no es una estaci√≥n completa) ---
        if not generate_all_for_station: # Solo mostrar si es generaci√≥n de √≠tem √∫nico, el otro ya tiene contexto general
            st.subheader("Contexto Adicional para el √çtem (Opcional - solo para √≠tem individual)")
            opcion_info_adicional = st.radio(
                "¬øDeseas proporcionar alguna informaci√≥n o contexto adicional para este √≠tem individual?",
                ("No", "S√≠"),
                key="info_ad_radio",
                horizontal=True
            )
            informacion_adicional_usuario = ""
            if opcion_info_adicional == "S√≠":
                informacion_adicional_usuario = st.text_area("Escribe aqu√≠ el contexto o la informaci√≥n que consideres relevante para la creaci√≥n del √≠tem:", key="info_ad_text")
        else:
            informacion_adicional_usuario = "" # No se usa si se genera por estaci√≥n

        # --- Nueva Secci√≥n: Usar Prompts Adicionales ---
        st.markdown("---")
        st.subheader("Personaliza con Prompts Adicionales (Opcional)")
        use_additional_prompts = st.checkbox("Activar Prompts Adicionales", help="Si activas esto, podr√°s a√±adir instrucciones espec√≠ficas para el generador y/o el auditor.")
        
        # Inicializar prompts adicionales
        prompt_bloom_adicional = ""
        prompt_construccion_adicional = ""
        prompt_especifico_adicional = ""
        prompt_auditor_adicional = ""

        if use_additional_prompts:
            col_gen_prompt, col_audit_prompt = st.columns(2)
            with col_gen_prompt:
                st.markdown("##### Prompts para el **Generador** de √çtems")
                
                # Opci√≥n 1: Prompts acerca de procesos cognitivos/Taxonom√≠a de Bloom
                use_bloom_prompt = st.checkbox("Prompts acerca de Procesos Cognitivos / Taxonom√≠a de Bloom", key="chk_bloom_prompt")
                if use_bloom_prompt:
                    prompt_bloom_adicional = st.text_area(
                        "Instrucciones para el generador sobre c√≥mo aplicar la Taxonom√≠a de Bloom (ej: 'El √≠tem debe requerir un an√°lisis profundo de causas y efectos', 'Aseg√∫rate de que el proceso cognitivo sea estrictamente de 'RECORDAR' y no de 'COMPRENDER'):", 
                        height=100, 
                        key="gen_bloom_prompt_text"
                    )

                # Opci√≥n 2: Prompts acerca de cosas generales de construcci√≥n
                use_construccion_prompt = st.checkbox("Prompts acerca de Reglas Generales de Construcci√≥n", key="chk_construccion_prompt")
                if use_construccion_prompt:
                    prompt_construccion_adicional = st.text_area(
                        "Instrucciones para el generador sobre el formato general o estilo (ej: 'Evita frases pasivas en el enunciado', 'Las opciones deben ser de longitud similar'):", 
                        height=100, 
                        key="gen_construccion_prompt_text"
                    )

                # Opci√≥n 3: Prompts acerca de cosas espec√≠ficas a tener en cuenta adicionales
                use_especifico_prompt = st.checkbox("Prompts acerca de Consideraciones Espec√≠ficas Adicionales", key="chk_especifico_prompt")
                if use_especifico_prompt:
                    prompt_especifico_adicional = st.text_area(
                        "Instrucciones muy espec√≠ficas o de √∫ltimo minuto para el generador (ej: 'El contexto debe mencionar una actividad deportiva', 'Incluye un personaje llamado 'Sof√≠a' en el enunciado'):", 
                        height=100, 
                        key="gen_especifico_prompt_text"
                    )

            with col_audit_prompt:
                st.markdown("##### Prompts para el **Auditor** de √çtems")
                prompt_auditor_adicional = st.text_area(
                    "Instrucciones espec√≠ficas para que la IA audite el √≠tem (ej: 'Verifica la coherencia en la numeraci√≥n', 'Aseg√∫rate que no haya ambig√ºedad en las opciones'):", 
                    height=200, 
                    key="audit_prompt_add"
                )

        st.markdown("---")
        # --- Selecci√≥n de Modelos para Generaci√≥n/Auditor√≠a ---
        st.subheader("Configuraci√≥n de Modelos de IA")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**Modelo para Generaci√≥n de √çtems**")
            gen_model_type = st.radio("Tipo de Modelo (Generaci√≥n)", ["Gemini", "GPT"], key="gen_model_type", index=0 if gemini_config_ok else 1) 
            gen_model_name = ""
            if gen_model_type == "Gemini":
                gen_model_name = st.selectbox("Nombre del Modelo Gemini (Generaci√≥n)", ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-1.5-pro", "gemini-1.5-flash"], key="gen_gemini_name")
            else: 
                gen_model_name = st.selectbox("Nombre del Modelo GPT (Generaci√≥n)", ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"], key="gen_gpt_name")
        
        with col2:
            st.markdown("**Modelo para Auditor√≠a de √çtems**")
            audit_model_type = st.radio("Tipo de Modelo (Auditor√≠a)", ["Gemini", "GPT"], key="audit_model_type", index=0 if gemini_config_ok else 1)
            audit_model_name = ""
            if audit_model_type == "Gemini":
                audit_model_name = st.selectbox("Nombre del Modelo Gemini (Auditor√≠a)", ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-1.5-pro", "gemini-1.5-flash"], key="audit_gemini_name")
            else: 
                audit_model_name = st.selectbox("Nombre del Modelo GPT (Auditor√≠a)", ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"], key="audit_gpt_name")

        # --- Bot√≥n para Generar y Auditar ---
        if st.button("Generar y Auditar √çtem(s)"): # Texto del bot√≥n actualizado
            if (gen_model_type == "Gemini" and not gemini_config_ok) or (gen_model_type == "GPT" and not openai_config_ok):
                st.error(f"Por favor, configura la API Key para el modelo de generaci√≥n ({gen_model_type}).")
            elif (audit_model_type == "Gemini" and not gemini_config_ok) or (audit_model_type == "GPT" and not openai_config_ok):
                st.error(f"Por favor, configura la API Key para el modelo de auditor√≠a ({audit_model_type}).")
            elif generate_all_for_station and context_option == "Yo quiero dar una idea del contexto general" and not contexto_general_estacion.strip():
                st.error("Por favor, introduce tu idea para el contexto general o selecciona que la IA lo genere.")
            else:
                st.markdown("---")
                st.info("Iniciando generaci√≥n y auditor√≠a del(los) √≠tem(s). Esto puede tardar unos momentos...")

                criterios_para_preguntas = {
                    "tipo_pregunta": "opci√≥n m√∫ltiple con 4 opciones",  
                    "dificultad": "media", 
                    "num_preguntas": 1,   
                    "contexto_educativo": "estudiantes de preparatoria (bachillerato)", 
                    "formato_justificacion": """
                         ‚Ä¢ Justificaci√≥n correcta: debe explicar el razonamiento o proceso cognitivo (NO por descarte).
                         ‚Ä¢ Justificaciones incorrectas: deben redactarse como: ‚ÄúEl estudiante podr√≠a escoger la opci√≥n X porque‚Ä¶ Sin embargo, esto es incorrecto porque‚Ä¶‚Äù
                    """
                }
                
                # Lista para almacenar todos los √≠tems procesados
                processed_items_list = []

                if generate_all_for_station:
                    st.subheader(f"Generando √≠tems para la Estaci√≥n: {estacion_seleccionada}")
                    
                    # Asegurar que se procesa una copia para no modificar el df original
                    unique_procesos = df_item_seleccionado[['PROCESO COGNITIVO', 'NANOHABILIDAD', 'MICROHABILIDAD', 'COMPETENCIA NANOHABILIDAD']].drop_duplicates().to_dict('records')
                    
                    if not unique_procesos:
                        st.warning(f"No se encontraron procesos cognitivos √∫nicos para la estaci√≥n '{estacion_seleccionada}'. No se generar√°n √≠tems.")
                    else:
                        progress_bar_text = st.empty()
                        progress_bar = st.progress(0)
                        
                        for i, item_spec_row in enumerate(unique_procesos):
                            # Construir una 'fila_datos' simulada para cada proceso cognitivo para la funci√≥n de generaci√≥n
                            current_fila_datos = {
                                'GRADO': grado_seleccionado,
                                '√ÅREA': area_seleccionada,
                                'ASIGNATURA': asignatura_seleccionada,
                                'ESTACI√ìN': estacion_seleccionada,
                                'PROCESO COGNITIVO': item_spec_row['PROCESO COGNITIVO'],
                                'NANOHABILIDAD': item_spec_row['NANOHABILIDAD'],
                                'MICROHABILIDAD': item_spec_row['MICROHABILIDAD'],
                                'COMPETENCIA NANOHABILIDAD': item_spec_row['COMPETENCIA NANOHABILIDAD']
                            }
                            
                            progress_text_msg = f"Procesando √≠tem {i+1} de {len(unique_procesos)}: {item_spec_row['PROCESO COGNITIVO']} - {item_spec_row['NANOHABILIDAD']}"
                            progress_bar_text.text(progress_text_msg)
                            progress_bar.progress((i + 1) / len(unique_procesos))

                            st.write(f"**Generando para:** {item_spec_row['PROCESO COGNITIVO']} - {item_spec_row['NANOHABILIDAD']}")

                            item_data = generar_pregunta_con_seleccion(
                                gen_model_type, gen_model_name, audit_model_type, audit_model_name, 
                                fila_datos=current_fila_datos,   
                                criterios_generacion=criterios_para_preguntas,
                                manual_reglas_texto=manual_reglas_texto,
                                informacion_adicional_usuario=informacion_adicional_usuario,
                                prompt_bloom_adicional=prompt_bloom_adicional, 
                                prompt_construccion_adicional=prompt_construccion_adicional, 
                                prompt_especifico_adicional=prompt_especifico_adicional, 
                                prompt_auditor_adicional=prompt_auditor_adicional,
                                contexto_general_estacion=contexto_general_estacion if context_option == "Yo quiero dar una idea del contexto general" else "" # Pasa el contexto definido por el usuario si aplica
                            )
                            if item_data:
                                processed_items_list.append(item_data)
                            st.markdown("---") # Separador visual entre √≠tems generados
                        
                        progress_bar_text.text("Todos los √≠tems han sido procesados.")
                        progress_bar.progress(1.0)
                        st.success(f"Se han procesado {len(processed_items_list)} √≠tems para la estaci√≥n '{estacion_seleccionada}'.")
                        
                        # Guardar la lista completa para su posterior revisi√≥n
                        st.session_state['processed_items_list_for_review'] = processed_items_list
                        if processed_items_list:
                            st.session_state['current_review_index'] = 0
                            st.session_state['awaiting_review'] = True
                else: # Generaci√≥n de un solo √≠tem
                    st.subheader(f"Generando √≠tem individual para: {proceso_cognitivo_seleccionado} - {nanohabilidad_seleccionada}")
                    item_data = generar_pregunta_con_seleccion(
                        gen_model_type, gen_model_name, audit_model_type, audit_model_name, 
                        fila_datos=df_item_seleccionado.iloc[0],   
                        criterios_generacion=criterios_para_preguntas,
                        manual_reglas_texto=manual_reglas_texto,
                        informacion_adicional_usuario=informacion_adicional_usuario,
                        prompt_bloom_adicional=prompt_bloom_adicional, 
                        prompt_construccion_adicional=prompt_construccion_adicional, 
                        prompt_especifico_adicional=prompt_especifico_adicional, 
                        prompt_auditor_adicional=prompt_auditor_adicional,
                        contexto_general_estacion="" # No hay contexto de estaci√≥n si es √≠tem individual
                    )
                    if item_data:
                        st.session_state['processed_items_list_for_review'] = [item_data]
                        st.session_state['current_review_index'] = 0
                        st.session_state['awaiting_review'] = True
                    else:
                        st.error("No se pudo generar el √≠tem bajo las condiciones seleccionadas.")

# --- L√≥gica para mostrar la interfaz de revisi√≥n ---
if 'awaiting_review' in st.session_state and st.session_state['awaiting_review']:
    
    if 'approved_items' not in st.session_state:
        st.session_state['approved_items'] = []
    
    current_index = st.session_state.get('current_review_index', 0)
    items_to_review = st.session_state.get('processed_items_list_for_review', [])
    
    if current_index >= len(items_to_review):
        st.success("¬°Has revisado todos los √≠tems! Ahora puedes descargarlos en la secci√≥n de 'Exportar Resultados'.")
        st.session_state['awaiting_review'] = False
        st.session_state['current_review_index'] = 0
        del st.session_state['processed_items_list_for_review']
        st.rerun()
        
    item_to_review = items_to_review[current_index]

    st.markdown("---")
    st.header(f"Revisi√≥n de √çtem ({current_index + 1} de {len(items_to_review)})")
    st.info(f"Dictamen de la Auditor√≠a Inicial: **{item_to_review['final_audit_status']}**")
    
    st.markdown("---")
    st.markdown("### √çtem Generado:")
    st.markdown(item_to_review['item_text'])
    
    st.markdown("---")
    st.markdown("### Observaciones de la Auditor√≠a:")
    st.markdown(item_to_review['final_audit_observations'])
    
    col_aprob, col_rechazo = st.columns(2)

    with col_aprob:
        if st.button("‚úÖ Aprobar y Siguiente"):
            st.session_state['approved_items'].append(item_to_review)
            st.session_state['current_review_index'] += 1
            st.rerun()

    with col_rechazo:
        if st.button("‚ùå Rechazar y Reintentar"):
            st.session_state['show_feedback_form'] = True
            
    if st.session_state.get('show_feedback_form', False):
        st.markdown("---")
        st.markdown("#### Por favor, proporciona tus observaciones para refinar el √≠tem:")
        
        feedback_enunciado = st.text_area("1. Observaciones del enunciado/contexto:", key="feedback_enunciado")
        feedback_opciones = st.text_area("2. Observaciones de las opciones de respuesta:", key="feedback_opciones")
        
        # Aqu√≠ construimos el prompt de feedback completo
        feedback_completo = ""
        if feedback_enunciado:
            feedback_completo += f"Observaciones sobre el enunciado/contexto: {feedback_enunciado}\n"
        if feedback_opciones:
            feedback_completo += f"Observaciones sobre las opciones de respuesta: {feedback_opciones}\n"

        if st.button("üîÑ Refinar con estas Observaciones"):
            st.info("Re-generando el √≠tem con tu feedback...")
            
            # Recuperar la fila de datos original
            original_fila_datos = item_to_review['classification']
            
            # Volver a llamar a la funci√≥n de generaci√≥n con el feedback del usuario
            refined_item_data = generar_pregunta_con_seleccion(
                gen_model_type, gen_model_name, audit_model_type, audit_model_name,
                fila_datos=original_fila_datos,
                criterios_generacion={"tipo_pregunta": "opci√≥n m√∫ltiple con 4 opciones", "dificultad": "media", "num_preguntas": 1, "contexto_educativo": "estudiantes de preparatoria (bachillerato)"},
                manual_reglas_texto=manual_reglas_texto,
                informacion_adicional_usuario=informacion_adicional_usuario,
                prompt_bloom_adicional=prompt_bloom_adicional,
                prompt_construccion_adicional=prompt_construccion_adicional,
                prompt_especifico_adicional=prompt_especifico_adicional,
                prompt_auditor_adicional=prompt_auditor_adicional,
                contexto_general_estacion=contexto_general_estacion,
                feedback_usuario=feedback_completo # Pasar el feedback al generador
            )
            
            if refined_item_data:
                # Reemplazar el √≠tem actual con el refinado
                st.session_state['processed_items_list_for_review'][current_index] = refined_item_data
                st.session_state['show_feedback_form'] = False
                st.success("¬°√çtem refinado exitosamente! Por favor, rev√≠salo de nuevo.")
                st.rerun()
            else:
                st.error("Fallo al refinar el √≠tem. Intenta de nuevo o ajusta tu feedback.")


# --- Secci√≥n de Exportaci√≥n a Word y descarga de Prompts (siempre visible al final de esta secci√≥n) ---
st.header("Exportar Resultados")

if 'approved_items' in st.session_state and st.session_state['approved_items']:
    num_items_processed = len(st.session_state['approved_items'])
    st.write(f"Hay **{num_items_processed}** √≠tem(s) aprobado(s) y disponible(s) para exportar.")
    
    # Exportar a Word
    nombre_archivo_word = st.text_input("Ingresa el nombre deseado para el archivo Word (sin la extensi√≥n .docx):", 
                                        f"items_{estacion_seleccionada.replace(' ', '_')}_{grado_seleccionado}", 
                                        key="word_filename")
    if nombre_archivo_word:
        word_buffer = exportar_a_word(st.session_state['approved_items'])
        st.download_button(
            label="Descargar √çtem(s) en Documento Word",
            data=word_buffer,
            file_name=f"{nombre_archivo_word}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        st.info("Haz clic para descargar el archivo Word con el(los) √≠tem(s) y su(s) auditor√≠a(s).")
    else:
        st.warning("Por favor, ingresa un nombre para el archivo Word.")

    # Descargar Prompts Utilizados
    st.markdown("---")
    st.subheader("Descargar Prompts Utilizados")
    st.info("Puedes descargar un archivo TXT con los prompts completos que se enviaron a los modelos de IA para este(os) √≠tem(s).")
    
    # Construir el contenido del TXT con ambos prompts para todos los √≠tems
    combined_prompts_content = ""
    for i, item_data in enumerate(st.session_state['approved_items']):
        combined_prompts_content += f"--- PROMPT DETALLADO PARA √çTEM #{i+1} ---\n"
        combined_prompts_content += f"**Clasificaci√≥n:** Grado: {item_data['classification']['Grado']}, √Årea: {item_data['classification']['√Årea']}, Asignatura: {item_data['classification']['Asignatura']}, Estaci√≥n: {item_data['classification']['Estaci√≥n']}, Proceso Cognitivo: {item_data['classification']['Proceso Cognitivo']}, Nanohabilidad: {item_data['classification']['Nanohabilidad']}\n\n"
        combined_prompts_content += f"--- PROMPT COMPLETO ENVIADO AL GENERADOR ---\n"
        combined_prompts_content += f"{item_data.get('generation_prompt_used', 'No disponible')}\n\n"
        combined_prompts_content += f"--- PROMPT COMPLETO ENVIADO AL AUDITOR ---\n"
        combined_prompts_content += f"{item_data.get('auditor_prompt_used', 'No disponible')}\n\n"
        combined_prompts_content += "="*80 + "\n\n" # Separador entre prompts de √≠tems
    
    prompt_download_filename = st.text_input("Nombre para el archivo TXT de prompts (sin .txt):", f"prompts_{estacion_seleccionada.replace(' ', '_')}", key="prompt_txt_filename")
    if prompt_download_filename:
        st.download_button(
            label="Descargar Prompts como TXT",
            data=combined_prompts_content.encode('utf-8'),
            file_name=f"{prompt_download_filename}.txt",
            mime="text/plain"
        )
        st.info("Haz clic para descargar el archivo TXT con los prompts detallados de todos los √≠tems.")
    else:
        st.warning("Ingresa un nombre para el archivo de prompts.")
else:
    st.info("No hay √≠tems aprobados disponibles para exportar en este momento. Genera y audita √≠tem(s) para que est√©n disponibles aqu√≠.")
